{
  "metadata": {
    "name": "music_recommender",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import scala.collection.Map\r\nimport scala.collection.mutable.ArrayBuffer\r\nimport scala.util.Random\r\nimport org.apache.spark.broadcast.Broadcast\r\nimport org.apache.spark.ml.recommendation.{ALS, ALSModel}\r\nimport org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\r\nimport org.apache.spark.sql.functions._\r\n\r\nimport java.nio.file.Paths\r\nimport spark.implicits._\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val OS \u003d System.getProperty(\"os.name\").toLowerCase\r\nif (OS.contains(\"win\"))\r\n  System.setProperty(\"hadoop.home.dir\", Paths.get(\"winutils\").toAbsolutePath.toString)\r\nelse\r\n  System.setProperty(\"hadoop.home.dir\", \"/\")\r\n\r\nval spark \u003d SparkSession.builder().config(\"spark.master\", \"local[*]\").getOrCreate()\r\n// Optional, but may help avoid errors due to long lineage\r\n//spark.sparkContext.setCheckpointDir(\"./tmp\")\r\n\r\n//val base \u003d \"./audioscrobbler_data/\"\r\nval base \u003d \"gs://dataproc-staging-europe-west6-872288405326-yxz32br1/audioscrobbler_data/\"\r\n\r\nval rawUserArtistData \u003d spark.read.textFile(base + \"user_artist_data.txt\")\r\nval rawArtistData \u003d spark.read.textFile(base + \"artist_data.txt\")\r\nval rawArtistAlias \u003d spark.read.textFile(base + \"artist_alias.txt\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// show the 5 first line in the file userartistdata\r\nrawUserArtistData.take(5).foreach(println)\r\n\r\n// It makes sense to transform this to a data frame with columns named “user” and “artist,” because it then becomes simple to compute simple statistics like the maximum and minimum of both columns \r\nval userArtistDF \u003d rawUserArtistData.map { line \u003d\u003e\r\n  val Array(user, artist, _*) \u003d line.split(\u0027 \u0027)\r\n  (user.toInt, artist.toInt)\r\n}.toDF(\"user\", \"artist\")\r\n\r\n// ALS does not specify the type to use. but it works better with Int 32 so we have to be sure the max number existing in the dataset is bellow the maximum int value (2147483647). Here it\u0027s below so we have no problems\r\nuserArtistDF.agg(min(\"user\"), max(\"user\"), min(\"artist\"), max(\"artist\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\r\nTo be able to interpret the futur results, it\u0027s useful to have a corespondance between ID and band Name. This information is given to us by the file artist_data which contain the ID Name map\r\nUse of flatmap insted of map because some line in the file are corrupted. \r\n*/\r\nval artistByID \u003d rawArtistData.flatMap { line \u003d\u003e\r\n      val (id, name) \u003d line.span(_ !\u003d \u0027\\t\u0027)\r\n      if (name.isEmpty) {\r\n        None\r\n      } else {\r\n        try {\r\n          Some((id.toInt, name.trim))\r\n        } catch {\r\n          case _: NumberFormatException \u003d\u003e None\r\n        }\r\n      }\r\n    }.toDF(\"id\", \"name\")\r\n \r\n// we can see that it map indeed the ID to the name of the artist   \r\nartistByID.head\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\r\nThe artist_alias.txt file maps artist IDs that may be misspelled or nonstandard to the\r\nID of the artist’s canonical name.\r\n\r\n*/\r\nval artistAlias \u003d rawArtistAlias.flatMap { line \u003d\u003e\r\n    val Array(artist, alias) \u003d line.split(\u0027\\t\u0027)\r\n    if (artist.isEmpty) {\r\n        None\r\n    } else {\r\n        Some((artist.toInt, alias.toInt))\r\n    }\r\n    }.collect().toMap\r\n    \r\nartistAlias.head\r\n\r\n// we can check different artist name mapped to the same canonical ID\r\n// for exemple we can see that 1208690 is mapped to 1003926, we can then see if the name they are refered to is the same.\r\nval (badID, goodID) \u003d artistAlias.head\r\nartistByID.filter($\"id\" isin (badID, goodID)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\nThis function transform all mispelled artist name ID to the right ID and the proceed to count the number of time a artist has been lisened to by a user\n*/\ndef buildCounts(rawUserArtistData: Dataset[String], bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame \u003d {\n    rawUserArtistData.map { line \u003d\u003e\n        val Array(userID, artistID, count) \u003d line.split(\u0027 \u0027).map(_.toInt)\n        val finalArtistID \u003d\n            bArtistAlias.value.getOrElse(artistID, artistID)\n    (userID, finalArtistID, count)\n    }.toDF(\"user\", \"artist\", \"count\")\n}\n\ndef buildArtistAlias(rawArtistAlias: Dataset[String]): Map[Int,Int] \u003d {\n    rawArtistAlias.flatMap { line \u003d\u003e\n    val Array(artist, alias) \u003d line.split(\u0027\\t\u0027)\n    if (artist.isEmpty) {\n        None\n    } else {\n        Some((artist.toInt, alias.toInt))\n    }\n    }.collect().toMap\n}\n\n/*\nThis is an optimisation. It ensure that Spark send and hold in memory just one copy for each executor in the cluster.\n*/\nval bArtistAlias \u003d spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\nval trainData \u003d buildCounts(rawUserArtistData, bArtistAlias)\n\ntrainData.cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\nDefine and Train the model with the basic parametre\n*/\nval model \u003d new ALS().\n    setSeed(Random.nextLong()).\n    setImplicitPrefs(true).\n    setRank(10).\n    setRegParam(0.01).\n    setAlpha(1.0).\n    setMaxIter(5).\n    setUserCol(\"user\").\n    setItemCol(\"artist\").\n    setRatingCol(\"count\").\n    setPredictionCol(\"prediction\").\n    fit(trainData)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\nThis display the feature vector\n*/\nmodel.userFactors.show(1, truncate \u003d false)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\nWe can see if the model make sense. (if the recommandation are good). To do this, first we need to see what our test user (2093760) is listening to.\n*/\nval userID \u003d 2093760\nval existingArtistIDs \u003d trainData.\n    filter($\"user\" \u003d\u003d\u003d userID).\n    select(\"artist\").as[Int].collect()\n    \n// after extraction of the artist ID that the user listen to. We can show their name.\nartistByID.filter($\"id\" isin (existingArtistIDs:_*)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\r\nThis function let us get the recommendation. The bad news is that, surprisingly, ALSModel does not have a method that directly computes top recommendations for a user. Its purpose is to estimate a user’s preference for any given artist. Spark 2.2 will add a recommendAll method to address this. This can be used to score all artists for a user and then return the few with the highest predicted score: \r\n*/\r\ndef makeRecommendations(model: ALSModel, userID: Int, howMany: Int): DataFrame \u003d {\r\n    // Select all artist IDs and pair with target user ID.\r\n    val toRecommend \u003d model.itemFactors.\r\n      select($\"id\".as(\"artist\")).\r\n      withColumn(\"user\", lit(userID))\r\n     \r\n    //  Score all artists, return top by score.\r\n    model.transform(toRecommend).\r\n      select(\"artist\", \"prediction\").\r\n      orderBy($\"prediction\".desc).\r\n      limit(howMany)\r\n  }"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\nShow the top recommendation for our test user but compare to his taste, we can see that it doesn\u0027t really match with the prediction.\n\n*/\nval topRecommendations \u003d makeRecommendations(model, userID, 5)\n//topRecommendations.show()\n\nval recommendedArtistIDs \u003d topRecommendations.select(\"artist\").as[Int].collect()\nartistByID.filter($\"id\" isin (recommendedArtistIDs:_*)).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def areaUnderCurve(positiveData: DataFrame, bAllArtistIDs: Broadcast[Array[Int]], predictFunction: (DataFrame \u003d\u003e DataFrame)): Double \u003d {\r\n\r\n    // What this actually computes is AUC, per user. The result is actually something\r\n    // that might be called \"mean AUC\".\r\n\r\n    // Take held-out data as the \"positive\".\r\n    // Make predictions for each of them, including a numeric score\r\n    val positivePredictions \u003d predictFunction(positiveData.select(\"user\", \"artist\")).\r\n      withColumnRenamed(\"prediction\", \"positivePrediction\")\r\n\r\n    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\r\n    // small AUC problems, and it would be inefficient, when a direct computation is available.\r\n\r\n    // Create a set of \"negative\" products for each user. These are randomly chosen\r\n    // from among all of the other artists, excluding those that are \"positive\" for the user.\r\n    val negativeData \u003d positiveData.select(\"user\", \"artist\").as[(Int,Int)].\r\n      groupByKey { case (user, _) \u003d\u003e user }.\r\n      flatMapGroups{ (userID:Int, userIDAndPosArtistIDs:Iterator[(Int,Int)]) \u003d\u003e\r\n        val random \u003d new Random()\r\n        val posItemIDSet \u003d userIDAndPosArtistIDs.map { case (_, artist) \u003d\u003e artist }.toSet\r\n        val negative \u003d new ArrayBuffer[Int]()\r\n        val allArtistIDs \u003d bAllArtistIDs.value\r\n        var i \u003d 0\r\n        // Make at most one pass over all artists to avoid an infinite loop.\r\n        // Also stop when number of negative equals positive set size\r\n        while (i \u003c allArtistIDs.length \u0026\u0026 negative.size \u003c posItemIDSet.size) {\r\n          val artistID \u003d allArtistIDs(random.nextInt(allArtistIDs.length))\r\n          // Only add new distinct IDs\r\n          if (!posItemIDSet.contains(artistID)) {\r\n            negative +\u003d artistID\r\n          }\r\n          i +\u003d 1\r\n        }\r\n        // Return the set with user ID added back\r\n        negative.map(artistID \u003d\u003e (userID, artistID))\r\n      }.toDF(\"user\", \"artist\")\r\n\r\n    // Make predictions on the rest:\r\n    val negativePredictions \u003d predictFunction(negativeData).\r\n      withColumnRenamed(\"prediction\", \"negativePrediction\")\r\n\r\n    // Join positive predictions to negative predictions by user, only.\r\n    // This will result in a row for every possible pairing of positive and negative\r\n    // predictions within each user.\r\n    val joinedPredictions \u003d positivePredictions.join(negativePredictions, \"user\").\r\n      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\r\n\r\n    // Count the number of pairs per user\r\n    val allCounts \u003d joinedPredictions.\r\n      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\r\n      select(\"user\", \"total\")\r\n    // Count the number of correctly ordered pairs per user\r\n    val correctCounts \u003d joinedPredictions.\r\n      filter($\"positivePrediction\" \u003e $\"negativePrediction\").\r\n      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\r\n      select(\"user\", \"correct\")\r\n\r\n    // Combine these, compute their ratio, and average over all users\r\n    val meanAUC \u003d allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\r\n      select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\r\n      agg(mean(\"auc\")).\r\n      as[Double].first()\r\n\r\n    joinedPredictions.unpersist()\r\n\r\n    meanAUC\r\n  }"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def predictMostListened(train: DataFrame)(allData: DataFrame): DataFrame \u003d {\r\n    val listenCounts \u003d train.groupBy(\"artist\").\r\n      agg(sum(\"count\").as(\"prediction\")).\r\n      select(\"artist\", \"prediction\")\r\n    allData.\r\n      join(listenCounts, Seq(\"artist\"), \"left_outer\").\r\n      select(\"user\", \"artist\", \"prediction\")\r\n  }"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\nval bArtistAlias \u003d spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\r\n\r\nval allData \u003d buildCounts(rawUserArtistData, bArtistAlias)\r\nval Array(trainData, cvData) \u003d allData.randomSplit(Array(0.9, 0.1))\r\ntrainData.cache()\r\ncvData.cache()\r\n\r\nval allArtistIDs \u003d allData.select(\"artist\").as[Int].distinct().collect()\r\nval bAllArtistIDs \u003d spark.sparkContext.broadcast(allArtistIDs)\r\n\r\nval mostListenedAUC \u003d areaUnderCurve(cvData, bAllArtistIDs, predictMostListened(trainData))\r\nprintln(mostListenedAUC)\r\n\r\nval evaluations \u003d\r\n  for (rank     \u003c- Seq(5,  30);\r\n       regParam \u003c- Seq(1.0, 0.0001);\r\n       alpha    \u003c- Seq(1.0, 40.0))\r\n  yield {\r\n    val model \u003d new ALS().\r\n      setSeed(Random.nextLong()).\r\n      setImplicitPrefs(true).\r\n      setRank(rank).setRegParam(regParam).\r\n      setAlpha(alpha).setMaxIter(20).\r\n      setUserCol(\"user\").setItemCol(\"artist\").\r\n      setRatingCol(\"count\").setPredictionCol(\"prediction\").\r\n      fit(trainData)\r\n\r\n    val auc \u003d areaUnderCurve(cvData, bAllArtistIDs, model.transform)\r\n\r\n    model.userFactors.unpersist()\r\n    model.itemFactors.unpersist()\r\n\r\n    (auc, (rank, regParam, alpha))\r\n  }\r\n\r\nevaluations.sorted.reverse.foreach(println)\r\n\r\ntrainData.unpersist()\r\ncvData.unpersist()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\r\nWe ones again rerun the recommandation for the test user but this time with ALS set with the good hyperparametre.\r\nWe can see that the recommandation are more accurate this time\r\n*/\r\n\r\nval bArtistAlias \u003d spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\r\nval allData \u003d buildCounts(rawUserArtistData, bArtistAlias).cache()\r\n\r\nval model \u003d new ALS().\r\n  setSeed(Random.nextLong()).\r\n  setImplicitPrefs(true).\r\n  setRank(10).setRegParam(1.0).setAlpha(40.0).setMaxIter(20).\r\n  setUserCol(\"user\").setItemCol(\"artist\").\r\n  setRatingCol(\"count\").setPredictionCol(\"prediction\").\r\n  fit(allData)\r\nallData.unpersist()\r\n\r\nval userID \u003d 2093760\r\nval topRecommendations \u003d makeRecommendations(model, userID, 5)\r\n\r\nval recommendedArtistIDs \u003d topRecommendations.select(\"artist\").as[Int].collect()\r\nartistByID.join(spark.createDataset(recommendedArtistIDs).toDF(\"id\"), \"id\").\r\n  select(\"name\").show()\r\n\r\nmodel.userFactors.unpersist()\r\nmodel.itemFactors.unpersist()\r\n"
    }
  ]
}