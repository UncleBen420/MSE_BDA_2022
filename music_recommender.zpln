{
  "paragraphs": [
    {
      "text": "import scala.collection.Map\r\nimport scala.collection.mutable.ArrayBuffer\r\nimport scala.util.Random\r\nimport org.apache.spark.broadcast.Broadcast\r\nimport org.apache.spark.ml.recommendation.{ALS, ALSModel}\r\nimport org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\r\nimport org.apache.spark.sql.functions._\r\n\r\nimport java.nio.file.Paths\r\nimport spark.implicits._\r\n",
      "user": "anonymous",
      "dateUpdated": "2022-06-01T13:03:13+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import scala.collection.Map\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.Random\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.ml.recommendation.{ALS, ALSModel}\nimport org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\nimport org.apache.spark.sql.functions._\nimport java.nio.file.Paths\nimport spark.implicits._\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654087886859_1243086342",
      "id": "paragraph_1654087886859_1243086342",
      "dateCreated": "2022-06-01T12:51:26+0000",
      "dateStarted": "2022-06-01T13:03:13+0000",
      "dateFinished": "2022-06-01T13:03:16+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:553"
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088395034_184471560",
      "id": "paragraph_1654088395034_184471560",
      "dateCreated": "2022-06-01T12:59:55+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1040",
      "text": "def buildArtistByID(rawArtistData: Dataset[String]): DataFrame = {\r\n    rawArtistData.flatMap { line =>\r\n      val (id, name) = line.span(_ != '\\t')\r\n      if (name.isEmpty) {\r\n        None\r\n      } else {\r\n        try {\r\n          Some((id.toInt, name.trim))\r\n        } catch {\r\n          case _: NumberFormatException => None\r\n        }\r\n      }\r\n    }.toDF(\"id\", \"name\")\r\n  }",
      "dateUpdated": "2022-06-01T13:04:33+0000",
      "dateFinished": "2022-06-01T13:04:35+0000",
      "dateStarted": "2022-06-01T13:04:33+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mbuildArtistByID\u001b[0m: \u001b[1m\u001b[32m(rawArtistData: org.apache.spark.sql.Dataset[String])org.apache.spark.sql.DataFrame\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088365705_274280784",
      "id": "paragraph_1654088365705_274280784",
      "dateCreated": "2022-06-01T12:59:25+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:971",
      "text": "def buildCounts(rawUserArtistData: Dataset[String], bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {\r\n    rawUserArtistData.map { line =>\r\n      val Array(userID, artistID, count) = line.split(' ').map(_.toInt)\r\n      val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)\r\n      (userID, finalArtistID, count)\r\n    }.toDF(\"user\", \"artist\", \"count\")\r\n  }",
      "dateUpdated": "2022-06-01T13:04:38+0000",
      "dateFinished": "2022-06-01T13:04:39+0000",
      "dateStarted": "2022-06-01T13:04:38+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mbuildCounts\u001b[0m: \u001b[1m\u001b[32m(rawUserArtistData: org.apache.spark.sql.Dataset[String], bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,Int]])org.apache.spark.sql.DataFrame\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088776379_248868071",
      "id": "paragraph_1654088776379_248868071",
      "dateCreated": "2022-06-01T13:06:16+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1528",
      "text": "def buildArtistAlias(rawArtistAlias: Dataset[String]): Map[Int,Int] = {\r\n    rawArtistAlias.flatMap { line =>\r\n      val Array(artist, alias) = line.split('\\t')\r\n      if (artist.isEmpty) {\r\n        None\r\n      } else {\r\n        Some((artist.toInt, alias.toInt))\r\n      }\r\n    }.collect().toMap\r\n  }",
      "dateUpdated": "2022-06-01T13:06:21+0000",
      "dateFinished": "2022-06-01T13:06:22+0000",
      "dateStarted": "2022-06-01T13:06:21+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mbuildArtistAlias\u001b[0m: \u001b[1m\u001b[32m(rawArtistAlias: org.apache.spark.sql.Dataset[String])scala.collection.Map[Int,Int]\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654087895269_577467813",
      "id": "paragraph_1654087895269_577467813",
      "dateCreated": "2022-06-01T12:51:35+0000",
      "status": "FINISHED",
      "$$hashKey": "object:554",
      "text": "  def preparation(rawUserArtistData: Dataset[String], rawArtistData: Dataset[String], rawArtistAlias: Dataset[String]): Unit = {\r\n    rawUserArtistData.take(5).foreach(println)\r\n\r\n    val userArtistDF = rawUserArtistData.map { line =>\r\n      val Array(user, artist, _*) = line.split(' ')\r\n      (user.toInt, artist.toInt)\r\n    }.toDF(\"user\", \"artist\")\r\n\r\n    userArtistDF.agg(min(\"user\"), max(\"user\"), min(\"artist\"), max(\"artist\")).show()\r\n\r\n    val artistByID = buildArtistByID(rawArtistData)\r\n    val artistAlias = buildArtistAlias(rawArtistAlias)\r\n\r\n    val (badID, goodID) = artistAlias.head\r\n    artistByID.filter($\"id\" isin (badID, goodID)).show()\r\n  }",
      "dateUpdated": "2022-06-01T13:06:25+0000",
      "dateFinished": "2022-06-01T13:06:26+0000",
      "dateStarted": "2022-06-01T13:06:25+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mpreparation\u001b[0m: \u001b[1m\u001b[32m(rawUserArtistData: org.apache.spark.sql.Dataset[String], rawArtistData: org.apache.spark.sql.Dataset[String], rawArtistAlias: org.apache.spark.sql.Dataset[String])Unit\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088425174_1855970860",
      "id": "paragraph_1654088425174_1855970860",
      "dateCreated": "2022-06-01T13:00:25+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1109",
      "text": "def makeRecommendations(model: ALSModel, userID: Int, howMany: Int): DataFrame = {\r\n    val toRecommend = model.itemFactors.\r\n      select($\"id\".as(\"artist\")).\r\n      withColumn(\"user\", lit(userID))\r\n    model.transform(toRecommend).\r\n      select(\"artist\", \"prediction\").\r\n      orderBy($\"prediction\".desc).\r\n      limit(howMany)\r\n  }",
      "dateUpdated": "2022-06-01T13:06:59+0000",
      "dateFinished": "2022-06-01T13:07:00+0000",
      "dateStarted": "2022-06-01T13:06:59+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mmakeRecommendations\u001b[0m: \u001b[1m\u001b[32m(model: org.apache.spark.ml.recommendation.ALSModel, userID: Int, howMany: Int)org.apache.spark.sql.DataFrame\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088111549_1873433594",
      "id": "paragraph_1654088111549_1873433594",
      "dateCreated": "2022-06-01T12:55:11+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:758",
      "text": "  def model(rawUserArtistData: Dataset[String], rawArtistData: Dataset[String], rawArtistAlias: Dataset[String]): Unit = {\r\n\r\n    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\r\n\r\n    val trainData = buildCounts(rawUserArtistData, bArtistAlias).cache()\r\n\r\n    val model = new ALS().\r\n      setSeed(Random.nextLong()).\r\n      setImplicitPrefs(true).\r\n      setRank(10).\r\n      setRegParam(0.01).\r\n      setAlpha(1.0).\r\n      setMaxIter(5).\r\n      setUserCol(\"user\").\r\n      setItemCol(\"artist\").\r\n      setRatingCol(\"count\").\r\n      setPredictionCol(\"prediction\").\r\n      fit(trainData)\r\n\r\n    trainData.unpersist()\r\n\r\n    model.userFactors.select(\"features\").show(truncate = false)\r\n\r\n    val userID = 2093760\r\n\r\n    val existingArtistIDs = trainData.\r\n      filter($\"user\" === userID).\r\n      select(\"artist\").as[Int].collect()\r\n\r\n    val artistByID = buildArtistByID(rawArtistData)\r\n\r\n    artistByID.filter($\"id\" isin (existingArtistIDs:_*)).show()\r\n\r\n    val topRecommendations = makeRecommendations(model, userID, 5)\r\n    topRecommendations.show()\r\n\r\n    val recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\r\n\r\n    artistByID.filter($\"id\" isin (recommendedArtistIDs:_*)).show()\r\n\r\n    model.userFactors.unpersist()\r\n    model.itemFactors.unpersist()\r\n  }",
      "dateUpdated": "2022-06-01T13:07:02+0000",
      "dateFinished": "2022-06-01T13:07:03+0000",
      "dateStarted": "2022-06-01T13:07:02+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mmodel\u001b[0m: \u001b[1m\u001b[32m(rawUserArtistData: org.apache.spark.sql.Dataset[String], rawArtistData: org.apache.spark.sql.Dataset[String], rawArtistAlias: org.apache.spark.sql.Dataset[String])Unit\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088978722_996551609",
      "id": "paragraph_1654088978722_996551609",
      "dateCreated": "2022-06-01T13:09:38+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1904",
      "text": "def predictMostListened(train: DataFrame)(allData: DataFrame): DataFrame = {\r\n    val listenCounts = train.groupBy(\"artist\").\r\n      agg(sum(\"count\").as(\"prediction\")).\r\n      select(\"artist\", \"prediction\")\r\n    allData.\r\n      join(listenCounts, Seq(\"artist\"), \"left_outer\").\r\n      select(\"user\", \"artist\", \"prediction\")\r\n  }",
      "dateUpdated": "2022-06-01T13:09:44+0000",
      "dateFinished": "2022-06-01T13:09:45+0000",
      "dateStarted": "2022-06-01T13:09:44+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mpredictMostListened\u001b[0m: \u001b[1m\u001b[32m(train: org.apache.spark.sql.DataFrame)(allData: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088445979_1633746205",
      "id": "paragraph_1654088445979_1633746205",
      "dateCreated": "2022-06-01T13:00:45+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1178",
      "text": "def areaUnderCurve(positiveData: DataFrame, bAllArtistIDs: Broadcast[Array[Int]], predictFunction: (DataFrame => DataFrame)): Double = {\r\n\r\n    // What this actually computes is AUC, per user. The result is actually something\r\n    // that might be called \"mean AUC\".\r\n\r\n    // Take held-out data as the \"positive\".\r\n    // Make predictions for each of them, including a numeric score\r\n    val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\r\n      withColumnRenamed(\"prediction\", \"positivePrediction\")\r\n\r\n    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\r\n    // small AUC problems, and it would be inefficient, when a direct computation is available.\r\n\r\n    // Create a set of \"negative\" products for each user. These are randomly chosen\r\n    // from among all of the other artists, excluding those that are \"positive\" for the user.\r\n    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\r\n      groupByKey { case (user, _) => user }.\r\n      flatMapGroups{ (userID:Int, userIDAndPosArtistIDs:Iterator[(Int,Int)]) =>\r\n        val random = new Random()\r\n        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\r\n        val negative = new ArrayBuffer[Int]()\r\n        val allArtistIDs = bAllArtistIDs.value\r\n        var i = 0\r\n        // Make at most one pass over all artists to avoid an infinite loop.\r\n        // Also stop when number of negative equals positive set size\r\n        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\r\n          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\r\n          // Only add new distinct IDs\r\n          if (!posItemIDSet.contains(artistID)) {\r\n            negative += artistID\r\n          }\r\n          i += 1\r\n        }\r\n        // Return the set with user ID added back\r\n        negative.map(artistID => (userID, artistID))\r\n      }.toDF(\"user\", \"artist\")\r\n\r\n    // Make predictions on the rest:\r\n    val negativePredictions = predictFunction(negativeData).\r\n      withColumnRenamed(\"prediction\", \"negativePrediction\")\r\n\r\n    // Join positive predictions to negative predictions by user, only.\r\n    // This will result in a row for every possible pairing of positive and negative\r\n    // predictions within each user.\r\n    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\r\n      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\r\n\r\n    // Count the number of pairs per user\r\n    val allCounts = joinedPredictions.\r\n      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\r\n      select(\"user\", \"total\")\r\n    // Count the number of correctly ordered pairs per user\r\n    val correctCounts = joinedPredictions.\r\n      filter($\"positivePrediction\" > $\"negativePrediction\").\r\n      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\r\n      select(\"user\", \"correct\")\r\n\r\n    // Combine these, compute their ratio, and average over all users\r\n    val meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\r\n      select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\r\n      agg(mean(\"auc\")).\r\n      as[Double].first()\r\n\r\n    joinedPredictions.unpersist()\r\n\r\n    meanAUC\r\n  }",
      "dateUpdated": "2022-06-01T13:09:47+0000",
      "dateFinished": "2022-06-01T13:09:48+0000",
      "dateStarted": "2022-06-01T13:09:47+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mareaUnderCurve\u001b[0m: \u001b[1m\u001b[32m(positiveData: org.apache.spark.sql.DataFrame, bAllArtistIDs: org.apache.spark.broadcast.Broadcast[Array[Int]], predictFunction: org.apache.spark.sql.DataFrame => org.apache.spark.sql.DataFrame)Double\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088260472_766982229",
      "id": "paragraph_1654088260472_766982229",
      "dateCreated": "2022-06-01T12:57:40+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:833",
      "text": "def evaluate(rawUserArtistData: Dataset[String], rawArtistAlias: Dataset[String]): Unit = {\r\n\r\n    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\r\n\r\n    val allData = buildCounts(rawUserArtistData, bArtistAlias)\r\n    val Array(trainData, cvData) = allData.randomSplit(Array(0.9, 0.1))\r\n    trainData.cache()\r\n    cvData.cache()\r\n\r\n    val allArtistIDs = allData.select(\"artist\").as[Int].distinct().collect()\r\n    val bAllArtistIDs = spark.sparkContext.broadcast(allArtistIDs)\r\n\r\n    val mostListenedAUC = areaUnderCurve(cvData, bAllArtistIDs, predictMostListened(trainData))\r\n    println(mostListenedAUC)\r\n\r\n    val evaluations =\r\n      for (rank     <- Seq(5,  30);\r\n           regParam <- Seq(1.0, 0.0001);\r\n           alpha    <- Seq(1.0, 40.0))\r\n      yield {\r\n        val model = new ALS().\r\n          setSeed(Random.nextLong()).\r\n          setImplicitPrefs(true).\r\n          setRank(rank).setRegParam(regParam).\r\n          setAlpha(alpha).setMaxIter(20).\r\n          setUserCol(\"user\").setItemCol(\"artist\").\r\n          setRatingCol(\"count\").setPredictionCol(\"prediction\").\r\n          fit(trainData)\r\n\r\n        val auc = areaUnderCurve(cvData, bAllArtistIDs, model.transform)\r\n\r\n        model.userFactors.unpersist()\r\n        model.itemFactors.unpersist()\r\n\r\n        (auc, (rank, regParam, alpha))\r\n      }\r\n\r\n    evaluations.sorted.reverse.foreach(println)\r\n\r\n    trainData.unpersist()\r\n    cvData.unpersist()\r\n  }",
      "dateUpdated": "2022-06-01T13:09:52+0000",
      "dateFinished": "2022-06-01T13:09:53+0000",
      "dateStarted": "2022-06-01T13:09:52+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mevaluate\u001b[0m: \u001b[1m\u001b[32m(rawUserArtistData: org.apache.spark.sql.Dataset[String], rawArtistAlias: org.apache.spark.sql.Dataset[String])Unit\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088276016_1116145703",
      "id": "paragraph_1654088276016_1116145703",
      "dateCreated": "2022-06-01T12:57:56+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:902",
      "text": "def recommend(rawUserArtistData: Dataset[String], rawArtistData: Dataset[String], rawArtistAlias: Dataset[String]): Unit = {\r\n\r\n    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\r\n    val allData = buildCounts(rawUserArtistData, bArtistAlias).cache()\r\n    val model = new ALS().\r\n      setSeed(Random.nextLong()).\r\n      setImplicitPrefs(true).\r\n      setRank(10).setRegParam(1.0).setAlpha(40.0).setMaxIter(20).\r\n      setUserCol(\"user\").setItemCol(\"artist\").\r\n      setRatingCol(\"count\").setPredictionCol(\"prediction\").\r\n      fit(allData)\r\n    allData.unpersist()\r\n\r\n    val userID = 2093760\r\n    val topRecommendations = makeRecommendations(model, userID, 5)\r\n\r\n    val recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\r\n    val artistByID = buildArtistByID(rawArtistData)\r\n    artistByID.join(spark.createDataset(recommendedArtistIDs).toDF(\"id\"), \"id\").\r\n      select(\"name\").show()\r\n\r\n    model.userFactors.unpersist()\r\n    model.itemFactors.unpersist()\r\n  }",
      "dateUpdated": "2022-06-01T13:09:57+0000",
      "dateFinished": "2022-06-01T13:09:58+0000",
      "dateStarted": "2022-06-01T13:09:57+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrecommend\u001b[0m: \u001b[1m\u001b[32m(rawUserArtistData: org.apache.spark.sql.Dataset[String], rawArtistData: org.apache.spark.sql.Dataset[String], rawArtistAlias: org.apache.spark.sql.Dataset[String])Unit\u001b[0m\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654088573847_1643787576",
      "id": "paragraph_1654088573847_1643787576",
      "dateCreated": "2022-06-01T13:02:53+0000",
      "status": "ERROR",
      "focus": true,
      "$$hashKey": "object:1247",
      "text": "val OS = System.getProperty(\"os.name\").toLowerCase\r\nif (OS.contains(\"win\"))\r\n  System.setProperty(\"hadoop.home.dir\", Paths.get(\"winutils\").toAbsolutePath.toString)\r\nelse\r\n  System.setProperty(\"hadoop.home.dir\", \"/\")\r\n\r\nval spark = SparkSession.builder().config(\"spark.master\", \"local[*]\").getOrCreate()\r\n// Optional, but may help avoid errors due to long lineage\r\nspark.sparkContext.setCheckpointDir(\"./tmp\")\r\n\r\nval base = \"./\"\r\nval rawUserArtistData = spark.read.textFile(base + \"user_artist_data.txt\")\r\nval rawArtistData = spark.read.textFile(base + \"artist_data.txt\")\r\nval rawArtistAlias = spark.read.textFile(base + \"artist_alias.txt\")\r\n\r\n\r\npreparation(rawUserArtistData, rawArtistData, rawArtistAlias)\r\nmodel(rawUserArtistData, rawArtistData, rawArtistAlias)\r\nevaluate(rawUserArtistData, rawArtistAlias)\r\nrecommend(rawUserArtistData, rawArtistData, rawArtistAlias)",
      "dateUpdated": "2022-06-01T13:12:57+0000",
      "dateFinished": "2022-06-01T13:12:58+0000",
      "dateStarted": "2022-06-01T13:12:57+0000",
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.sql.AnalysisException: Path does not exist: file:/opt/zeppelin/user_artist_data.txt\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:806)\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:803)\n  at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)\n  at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n  at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n  at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n  at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)\n"
          }
        ]
      }
    },
    {
      "user": "anonymous",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654089001685_1793355308",
      "id": "paragraph_1654089001685_1793355308",
      "dateCreated": "2022-06-01T13:10:01+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:2116"
    }
  ],
  "name": "music_recommender",
  "id": "2H4XAA311",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/music_recommender"
}